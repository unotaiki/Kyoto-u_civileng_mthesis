%!TEX root = ../main.tex
\chapter{背景理論}

\section{画像に基づく3次元復元 (3D Reconstruction from Images)}
画像を用いた測量は写真測量(Photogrammetry)と呼ばれ、古い歴史あり。
\url{https://duplicate-3d.com/rd/2025-09-11-photogrammetry-history/} この記事を参考に。
もともとは専門的な技能を持った人が、専用の機械(?)を用いて作成。

画像のデジタル化、イメージセンサの発明によりコンピュータビジョン（Computer Vision, CV）が始まる。
Image Sensorから得た視覚情報から、人間や他の生物と同じようにComputerやMachineにSceneを理解させる。

画像センサ（Image Sensor）が捉える情報は、本質的には3次元シーンから2次元平面への射影（Projection）である。
この過程において、3次元空間の奥行き情報は1次元分欠落し、情報の「縮退」が発生する。
3次元復元の主眼は、この失われた次元を幾何学的制約や事前知識（Priors）を用いて補完し、元のシーンの構造を逆問題として解くことにある。
3次元情報は生物が自己が生きる世界を認識、理解するうえで必須の情報であり、ロボットの自己位置推定、環境認識などにも多大なニーズがあり、Computer Visionの主要タスクとして数多くの研究がなされ、今もめちゃくちゃレッドオーシャン。

古くから、単眼画像から形状を推定する手法として、輝度やテクスチャ、影などの手掛かりを利用する \textit{Shape from X}（$X \in \{\text{shading, Silhouette, Texture, Focus, etc.}\}$）の研究が行われてきた。
しかし、より頑健な復元を行うためには、視点移動を伴う複数枚の画像、あるいは動画（Image sequence）を用いて幾何的な整合性を元に手法が主流となった。
この手法、タスクの総称を Structure from Motion (SfM) と呼ぶ。
このSfMを起点とする、3D Vision for Geometry手法の発達、高度なUIを備えた商用ソフト(Pix4D, Metashape, etc.)の出現、オープンソース化により、今日ではPhotogrammetry技術は広く一般に普及した。

\subsection{Structure from Motion}
SfMは、複数の視点から撮影された画像群に基づき、カメラの内部・外部パラメータ（三次元的な動き）と、シーンの疎な（Sparse）3次元構造を同時に推定する手法である。
Tomasi-Kanadeによる行列分解法に始まる。
HartleyやZissermanら(2000年代)によって幾何学的理論の基礎が確立された。 (VGGTのYoutubeの対談動画で取り上げられていた。もう少し詳しくは、Harley先生たちの本を参照。)
2016年に発表された COLMAP は、高い精度と汎用性、そしてOpen Sourceのプロジェクトとしての完成度から現在でもアカデミック分野でデファクトスタンダードとして広く利用されている。
\checkref{COLMAPでは、Pixel wiseなんちゃらで、MVSの新規研究も実装されている}
また、COLMAPにより出力される、正確なカメラ内部パラメータ(Intrinsic)と外部パラメータ(Extrinsic)は、歪みのない画像(Undistorted Images)は、後述するDense Reconstruction や 新規視点合成タスク のための入力データとして、重要なパイプラインの一角を担っている。

In a typical incremental SfM pipeline, keypoints are firstly detected and matched across frames using feature detectors and descriptors such as SIFT \parencite{Lowe2004_SIFT}.
Fundamental matrix $F$ between two images is then calculated, commonly via eight-points algorithm \parencite{Hartley1997_8PointAlgorithm} combined with random sample consensus (RANSAC) \parencite{Fischler1987_RANSAC}, which lead to camera pose recovery through singular value decomposition.
New camera poses are iteratively registered via Perspective-n-Point algorithms, which align estimated 3D points with 2D features in new frames.
Triangulation is subsequently applied to obtain additional 3D points from feature correspondences, and bundle adjustment (BA) \parencite{Triggs2000_BA} is finally performed to minimize reprojection error, refining both camera parameters and 3D points. 
画像を貼るぞい!!!http://theia-sfm.org/sfm.html

\begin{figure}[htbp]
  \centering
  \captionsetup{justification=raggedright}
  \includegraphics[width=0.7\linewidth]{figure/40_prelim/sfm_pipeline.png}
  \caption{SfM Pipeline \cite{fig:Theia-SfM}}
  \label{fig:sfm_pipeline}
\end{figure}

\subsection{Multi-View Stereo}
SfMなどによって得られたカメラパラメータが既知の多視点画像郡から、各画素単位で密な深度推定により、高密度な三次元復元を行うのが Multi-View Stereo (MVS) である。
SfMの出力三次元情報が疎な(Sparse)点群であるのに対して、MVSは物体表面の密な(Dense)な三次元点群やメッシュ(Mesh)を推定することから、MVSはDense 3D Reconstruction (密な三次元再構成)のタスクを行っていると言える。
PatchMatch法などのアルゴリズムを用い、各フレームにおける深度（Depth）や法線（Normal）の一貫性を最適化することで、精緻な幾何構造を構築する。
より詳しく解説したい。
ここれは、\cite{Furukawa2010_PatchMVS}が提案した、Patch-based Multi-View Stereo (PMVS)と、\cite{Schnberger2016ECCV_PatchMatchStereo}が提案した、PatchMatch Stereoを代表して解説する。

% --- PMVS ---
\textbf{PMVS}: 

\cite{Furukawa2010_PatchMVS}が提案した、Patch-based Multi-View Stereoの手法であり、現在まで続く物体表面の形状を密に3次元推定するための先駆的かつ代表的な手法である。
基本的な考えは、｢物体表面をパッチ(短形長方形)の集合で表現･推定｣することである。
あるパッチ$B$は、以下のパラメータを持つ:
\rewrite{パッチはpと表すのが適当に思えるが、後にGaussian中心もpと表しているので、ここではsと書いている。Gaussian中心は$\bm{\mu}$と書くのが良いのかもしれない。
→ パッチはB(ブロック)と書くことにした。sはScaleと混同する}

\begin{itemize}
  \item \textbf{中心座標} $\bm{p_B} \in \mathbb{R}^3$: パッチ$B$の中心の三次元空間上の位置。
  \item \textbf{法線} $\bm{n_B} \in \mathbb{R}^3$: パッチ$B$の法線。
  \item \textbf{参照画像} $R_B$: パッチ$B$を最も正面から捉える入力画像。
  \item \textbf{可視画像集合} $V_B$: パッチ$B$を捉える入力画像のリスト。
\end{itemize}

PMVSの処理は、大きく分けて、｢Initialization(初期化)｣、｢Expansion(拡張)｣、｢Filtering(フィルタリング)｣の三つのステージの反復だよ。

Initialization: SfMと同様に、各画像から特徴点検出･マッチングを行う\cite{Lowe2004_SIFT}。
\checkref{Harrisコーナー検出やDoGに関して、SfMの欄で説明しておくべき}
ここから得られる疎な点群をSeed Patchとして、初期の法線とともに最初のパッチ郡を生成するよ。
前処理としてカメラパラメータの取得にSfMを用いている場合、これらの特徴点と、Triangulation(三角測量)によって得られる疎な三次元点群はそのまま使用できる。
\note{初期の法線はカメラ方向などから推定するらしいがどのように推定する? 元論文の式(5)}

Expansion: PMVSでは、｢表面がある場所の隣には滑らかに連続する別の表面がある｣という世界を構成する三次元形状に関するPrior(前提知識)を仮定し、既存のパッチの周囲に新しいパッチを増殖させていく。
\begin{enumerate}
  \item \textbf{隣接探索(Identifying Cells for Expansion)}: 既存のパッチ$B$を画像上に投影した際に、その隣の領域にパッチがない場合新しいパッチ$B'$を生成する。
  \item \textbf{パラメータ継承} 新しいパッチ$B'$の深度と法線は$B$の親パッチのものを受け継ぎ初期化される。
  \item \textbf{最適化} パッチ位置$\bm{p_B'}$と法線$\bm{n_B'}$を、そのパッチを複数画像間での画素値･色の整合性(Photometric Consistency)を最大化するように最適化する。
\end{enumerate}
これにより、初期化時にパッチが存在しなかった領域にも徐々に面が張り出していき、密な復元が可能になる。
Photometricな一貫性を担保するために、｢参照画像$R_B$から見たパッチ$B$の模様｣と｢可視画像集合$V_B$におけるパッチ$B$の模様｣を比較する。
各パッチ$B$に対するPhotometric測定関数は以下のように表される:
\begin{equation}
  g(p) = \frac{1}{|V(B) \setminus R(B) |} \sum_{I \in V(B), I \neq R(B)} \text{NCC}(B, I, R(B))
\end{equation}
ここで、$NCC(B, I_1, I_2)$は、Computer Visionで一般的に用いられるパッチ間の類似度を測定するための指標である、画像$I_1$と$I_2$の間の正規化相互相関(Normalized Cross-Correlation)であり、以下のように表される:
\begin{equation}
  NCC(I_1, I_2) = \frac{\sum_{i,j} (I_1(i,j) - \bar{I_1})(I_2(i,j) - \bar{I_2})}{\sqrt{\sum_{i,j} (I_1(i,j) - \bar{I_1})^2 \sum_{i,j} (I_2(i,j) - \bar{I_2})^2}}
\end{equation}
ただし、$\bar{I_1}$と$\bar{I_2}$は、画像$I_1$と$I_2$の平均画素値である。
\note{合っているか確認。正直、式を明示的に示す必要もない気がする。}

この時、単に画像を比較するのではなく、パッチの法線 $n(B)$ を考慮して画像をホモグラフィ変換（射影変換）し、「正面から見た形」に補正してから比較します。
このスコアが高くなるように、共役勾配法などの最適化手法を用いて、パッチの 位置 $p(B)$ と 法線 $n(B)$ を反復的に修正するよ。
\note{共役勾配法とは?\url{https://en.wikipedia.org/wiki/Conjugate_gradient_method}}

Filtering: 拡張により誤って増殖したパッチやノイズを除去するよ。
あるカメラから見て、手前にあるパッチが奥のパッチを隠している場合、奥のパッチは幾何的に矛盾しているため削除するよ。
また、ごく少数のカメラからしか見えていいないパッチや、$NCC$スコアが低いパッチも削除するよ。


% --- PatchMatch Stereo ---
\textbf{PatchMatch Stereo}: 
PMVSが「確信度の高い種（Seed）を育てていく」のに対し、PatchMatch Stereoは「ランダムな推測から始め、隣接画素の良い推測結果を伝播（Propagate）させて全体を収束させる」という、全く逆のアプローチを取る。

これは元々、画像編集（Inpainting）のためにBarnesらが2009年に提案したアルゴリズムを、Bleyerらが2011年にStereoに、そして\cite{Schnberger2016ECCV_PatchMatchStereo}が2016年にMVSへと拡張したよ。

\checkref{理解する。本当にそうか?あと、論文と}
参照画像上のすべての画素 $\Gamma$ に対して、局所的な接平面（Local Tangent Plane）を推定する.
\note{Local Tangent Planeとは? またLocal Tangent Plane仮説とはどういうPrior?}
単なる「深度 $d$」のスカラー値推定ではなく、「深度 $d$ + 法線 $\mathbf{n}$」を推定するのが最大の特徴です。
これにより、斜めの面（Slanted surface）に対しても高いフォトメトリック整合性を確保できます。
画素 $\Gamma$ における平面パラメータ $\theta_\Gamma$ は以下のように定義される: $\theta_\Gamma = \{ d_\Gamma, \mathbf{n}_\Gamma \}$。
ここで、$d_\Gamma$ は深さ、$\mathbf{n}_\Gamma$ は法線ベクトルです。
この平面仮説を用いると、カメラ内部行列$K$を用いて、参照画像の画素 $\Gamma$ をソース画像へ投影する際のホモグラフィ $H_\Gamma$ が誘導されます（Plane-induced Homography）。
\begin{equation}
H_\Gamma = \mathbf{K} \left( \mathbf{R} - \frac{\mathbf{t} \mathbf{n}_\Gamma^T}{d_\Gamma} \right) \mathbf{K}^{-1}
\end{equation}
\note{ソース画像とは何?}
この $H_\Gamma$ を用いてパッチを変形させることで、視点変更に伴うパースペクティブ歪みを考慮した正確なマッチングコスト計算が可能になる。

Initialization（ランダム初期化）: 全ての画素に対して、深度と法線を完全なランダム値で初期化する。

Propagation（伝播）:「空間的一貫性（Spatial Consistency）」**を利用します。「ある画素の正しい平面パラメータは、その隣接画素の平面パラメータと似ている可能性が高い」という仮定に基づき、隣接画素のパラメータを自分のものとしてテスト。
画像を左上から右下へ、次は右下から左上へと走査します。比較: 現在の画素 $\Gamma$ のコストと、隣接画素（例えば左 $p_l$ と上 $p_u$）のパラメータ $\theta_{pl}, \theta_{pu}$ を $p$ に適用した時のコストを比較します。更新: もし隣接画素のパラメータの方がコストが低ければ、そのパラメータを自分のものとして採用（コピー）します。これにより、画像の端にある「正解に近い値」が、反復ごとに画像全体へ波紋のように広がっていきます。Step 3: View Selection（ビュー選択）COLMAPの実装（Pixelwise View Selection for Unstructured Multi-View Stereo）において重要なのが、画素ごとに最適なソース画像を選択する点です。全画像を使うのではなく、その画素の法線やオクルージョンを考慮し、フォトメトリックに最も信頼できるソース画像群を動的に選び出し、スコアを計算します。これによりロバスト性が飛躍的に向上します。Step 4: Refinement（摂動と微調整）Propagationだけでは、既存の値をコピーするだけで、新たな値（Sub-pixel精度）が生まれません。そこで、現在のパラメータ $\theta_p$ に微小なランダムノイズ（摂動）を加え、コストが下がるなら更新します。反復が進むにつれてノイズの範囲（探索半径）を小さくしていくことで、局所解へ収束させます。


\subsection{Feed Forward 3D Reconstruction}
\note{本筋とは直接繋がらないから、、消すかどうか?}
近年（2023-2025年現在、大規模データセットを用いた深層学習、特に Transformer アーキテクチャの導入により、従来の幾何学的手法を代替・補完する「フィードフォワード型」の手法が急速に台頭している。
従来手法が、Optimization-basedの手法であるのに対し、Feed Forwardの手法は、Data-drivenの手法である。

Depth Anything\cite{}: 膨大な画像データから学習された 3D Priors を活用することで、未知のシーンに対しても極めて精緻な単眼深度推定を実現している。

DUSt3R / MASt3R\cite{}: 従来の SfM/MVS のような複雑なパイプライン（特徴点抽出、マッチング、 bundle adjustment 等）を介さず、2枚の画像から直接 Point Map を回帰する。これにより、カメラパラメータを事前に与えることなく、エンドツーエンドでの3次元形状およびカメラ姿勢の推定が可能となった。

VGGT\cite{}: Best Paper Award at CVPR 2025.

現在、大規模モデルによるデータ駆動型のアプローチへと大きなパラダイムシフトを迎えている。

SfM、MVSなどはGeometric Opticsを前提とし、光線が直進するという普遍的で強い過程を前提に構築されており、
Feed Forward手法は、屈折のあるSceneが学習データには不足しており、屈折のあるSceneを正確な一貫性を持って復元できるかは疑問だ? 
(動画生成AIが流体や重力、光の反射といった物理法則を理解しつつある側面を考えると、いずれは屈折のあるSceneも学習データに含まれるようになるかもしれない。)

\missingfigure{Feed Forwardを表す図。と思ったが、これは本筋とほとんど関係ないんだよね}
