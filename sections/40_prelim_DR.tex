%!TEX root = ../main.tex

\section{Inverse Rendering as 3D Reconstruction}

\subsubsection{レンダリングの順問題と逆問題}
レンダリング（Rendering）とは、3次元モデルやシーンデータ（形状、質感、光源等）とカメラパラメータを入力とし、コンピュータグラフィックスの技術を用いて2次元画像を生成するプロセスを指す。
これに対し、画像からその生成元となった3次元シーンの構造を推定する試みは「逆レンダリング（Inverse Rendering）」と呼ばれる。
これは画像からの3次元構造を推定する手法の一種と捉えることができる。
従来のSfMやMVSといった手法は、画像間の幾何学的対応関係に基づいて3次元構造を推定する。
しかし、近年注目されているアプローチは、画像生成過程そのものをモデル化し、観測された画像とレンダリングされた画像の差異を最小化するように3次元シーンを最適化する手法である。


\subsubsection{微分可能レンダリングによる最適化}
逆レンダリングを勾配法（Gradient Descent）によって解くためには、レンダリングプロセスが微分可能（Differentiable）である必要がある。
(なんで勾配法で解く必要があるのか土木人に分かるように説明してあげないと。DLの発達とその最適化手法の根底にあることを言及)
微分可能レンダリング（Differentiable Rendering, DR）の枠組みでは、以下のループによって3次元シーンが最適化される。

\begin{enumerate}
    \item \textbf{レンダリング:} 現在推定されている3次元シーン（形状・外観）から、仮想的な視点で画像を生成する。
    \item \textbf{損失計算:} 生成された画像（Rendered Image）と、実際に撮影された正解画像（Source Image）との間の誤差（Loss）を計算する。
    \item \textbf{誤差逆伝播:} 微分可能なレンダラを介して、誤差の勾配（Gradient）を3次元シーンのパラメータへと連鎖律に従って伝播させる。
    \item \textbf{更新:} 勾配を用いて、3次元シーンのパラメータを逐次更新する。
\end{enumerate}

この手法は、幾何学的な特徴点のみならず、画像内の全画素の情報を最適化に利用できる
従来のBundle Adjustmentも2次元画像座標上の特徴点を介して、3次元シーンを構成する点群を再投影誤差を介し最小化するという点で、微分可能レンダリングの一種と捉えることができる。

\subsubsection{Novel View Synthesis}
従来のSfMやMVSの手法はGeometric Reconstructionとして、主に幾何情報の復元に焦点を当てていた。
実際のSceneは幾何情報(Geometry)に加え、照明(illumination)、テクスチャ(texture)、BRDFなどでモデル化される表面特性などを含む。
新規視点合成というタスクでは、ポーズを付与した画像から、その視点外からの新視点の画像を推定するタスクである。
SfMやMVSがGeometric Reconstructionとするならば、新規視点合成はAppearance Reconstructionとなる。
Appearanceには当然、Geometryの情報も含まれているため、Appearance ReconstructionはGeometry Reconstructionのタスクを暗に含んでいると言える。
図を挿入して上げるぞ。(CGのレンダリングプロセスと、ベン図)


\subsubsection{Neural Radiance Fields (NeRF)}
微分可能レンダリングの代表例が NeRF (Neural Radiance Fields) である。


NeRFでは、シーンをボクセルやポリゴンといった明示的な幾何構造ではなく、多層パーセプトロン（MLP）を用いた「連続的な輝度場（Radiance Field）」として表現する。
これは、点群やメッシュといった、シーンの幾何情報を直接的にエンコードするExplicit Representationではなく、関数を介して表現するImplicit Representationの一種である。

具体的には、空間上の座標 $(x, y, z)$ と視線方向 $(\theta, \phi)$ を入力とし、その点における放射輝度（RGB）と体積密度（$\sigma$）を出力する関数 $f_\Theta$ を学習する。
この表現を用いて、以下の Volumetric Renderingによりピクセル値を算出する。

\begin{equation}
    C(\mathbf{r}) = \int_{t_n}^{t_f} T(t) \sigma(\mathbf{r}(t)) \mathbf{c}(\mathbf{r}(t), \mathbf{d}) dt
\end{equation}

ここで $T(t)$ は透過率、$\mathbf{r}(t)$ はレイ上の点を表す。この積分プロセスは離散的なサンプリングによって近似され、微分可能な形で実装される。
ボリュメトリックレンダリングは、元来、煙や炎などの不均等な媒質を通過する光の伝播をモデル化を可視化するために開発されたものだが、メッシュなどのラスタライズに含まれる離散性がなく、微分可能性を保つとともに、汎用的なScene表現に用いられる。
これにより、複雑な光学的特性（反射や半透明など）を含むシーンにおいても、エンドツーエンドでの高精度な再構成が可能となった。

ピクセル色の決定には、古典的なボリュームレンダリングの原理が応用される。仮想的なカメラから放たれたレイ（光線）に沿って空間をサンプリングし、各サンプル点での密度 $\sigma$ を重みとした放射輝度 $\mathbf{c}$ の積分値を算出することで、最終的なピクセル値が決定される。
この積分計算は離散的な総和として近似されるが、演算過程の全てが微分可能に保たれているため、レンダリング画像と実画像の二乗誤差を損失関数とし、MLPの重みをエンドツーエンドで最適化できる。
しかし、NeRFは写真のような忠実度の自由視点合成を実現した一方で、実用上の重大な障壁も露呈させた。
1ピクセルの描画のためにレイに沿った数百回のMLP推論を要する計算負荷の高さは、リアルタイムレンダリングを困難にし、またシーンごとに日単位の学習を要する点は、大規模なデータセットへの適用を制限している。
また、3次元シーンがMLPにエンコードされる点で、その解釈や編集可能性が乏しく、測量用途としての適用性に欠ける。
これらの課題、すなわち「計算資源の集約性」と「暗黙的表現による編集の困難性」を克服しようとする動機が、後の明示的な幾何プリミティブを用いる手法への回帰を促すこととなった。
