%!TEX root = ../main.tex

\section{Inverse Rendering as 3D Reconstruction}

\subsubsection{Inverse Rendering}
\url{https://gemini.google.com/app/0bb4c009393122eb}

コンピュータグラフィックス（CG）の古典的な課題は、3次元のシーン記述（形状、材質、光源）から2次元の画像を生成することである。
この過程は「Forward Rendering（順方向レンダリング）」と呼ばれ、一般的にはレンダリングと呼ぶ。
これを数学的な関数 $R$ と見なすと、シーンパラメータ $\Theta$ から画像 $I$ への写像 $I = R(\Theta)$ として表現できる。
これに対し、Inverse Rendering（逆レンダリング） は、観測された画像 $I_{obs}$ から、それを生成したシーンパラメータ $\Theta$ を推定する逆問題 $\Theta = R^{-1}(I_{obs})$ を解くことである。
観測された画像郡から3Dシーンを推定するという意味で、Inverse Renderingは画像からの三次元再構成の一例であるといえる。

しかし、物理的な世界から画像への射影は情報の損失（奥行きの消失、法線とテクスチャの混同など）を伴うため、この逆問題は本質的に不良設定問題（Ill-posed problem）となる。
この困難な問題を、最適化手法を用いて解くための強力なフレームワークとして登場したのが、Differentiable Rendering（微分可能レンダリング） である。

\missingfigure{RenderingとInverse Renderingを表す図}


\subsection{Analysis-by-Synthesis and Novel View Synthesis}
現代のInverse Renderingの多くは、Analysis-by-Synthesis というアプローチを採用している。
これは、パラメータ $\Theta$ を直接回帰するのではなく、レンダリングされた画像 $R(\Theta)$ と観測画像 $I_{obs}$ との間の再構成誤差 $\mathcal{L}$ を最小化する最適化問題として定式化される。
\begin{equation}\label{eq:analysis-by-synthesis}
    \Theta^* = \underset{\Theta}{\text{argmin}} \space \mathcal{L}(R(\Theta), I_{obs})
\end{equation}
ここで $\mathcal{L}$ は損失関数 (L2ノルムやPerceptual Loss) である。
この最小化問題を勾配降下法（Gradient Descent）で解くためには、レンダリング関数 $R$ がパラメータ $\Theta$ に関して微分可能である必要がある。
すなわち、3Dシーンパラメータ $\Theta$ に関する損失関数 $\mathcal{L}$ の勾配 $\frac{\partial \mathcal{L}}{\partial \Theta}$ を計算できなければならない。
連鎖律（Chain Rule）を適用すると、勾梯は以下のように分解される。

\begin{equation}\label{eq:chain-rule-of-DR}
    \frac{\partial \mathcal{L}}{\partial \Theta} = \frac{\partial \mathcal{L}}{\partial I} \cdot \frac{\partial I}{\partial \Theta}
\end{equation}

前半の $\frac{\partial \mathcal{L}}{\partial I}$（画像の画素値に対するLossの勾配）は容易に計算できるが、後半の $\frac{\partial I}{\partial \Theta}$（シーンパラメータの変化が画素値にどう影響するか） の計算は一般的な三次元形状表現であるMeshでは難しい。
\fix{いらないと思う}

\cref{eq:analysis-by-synthesis}の定式化によって、






\subsubsection{微分可能レンダリングによる最適化}
逆レンダリングを勾配法（Gradient Descent）によって解くためには、レンダリングプロセスが微分可能（Differentiable）である必要がある。
(なんで勾配法で解く必要があるのか土木人に分かるように説明してあげないと。DLの発達とその最適化手法の根底にあることを言及)
微分可能レンダリング（Differentiable Rendering, DR）の枠組みでは、以下のループによって3次元シーンが最適化される。

\begin{enumerate}
    \item \textbf{レンダリング:} 現在推定されている3次元シーン（形状・外観）から、仮想的な視点で画像を生成する。
    \item \textbf{損失計算:} 生成された画像（Rendered Image）と、実際に撮影された正解画像（Source Image）との間の誤差（Loss）を計算する。
    \item \textbf{誤差逆伝播:} 微分可能なレンダラを介して、誤差の勾配（Gradient）を3次元シーンのパラメータへと連鎖律に従って伝播させる。
    \item \textbf{更新:} 勾配を用いて、3次元シーンのパラメータを逐次更新する。
\end{enumerate}

この手法は、幾何学的な特徴点のみならず、画像内の全画素の情報を最適化に利用できる
従来のBundle Adjustmentも2次元画像座標上の特徴点を介して、3次元シーンを構成する点群を再投影誤差を介し最小化するという点で、微分可能レンダリングの一種と捉えることができる。

\subsubsection{Novel View Synthesis}
従来のSfMやMVSの手法はGeometric Reconstructionとして、主に幾何情報の復元に焦点を当てていた。
実際のSceneは幾何情報(Geometry)に加え、照明(illumination)、テクスチャ(texture)、BRDFなどでモデル化される表面特性などを含む。
新規視点合成というタスクでは、ポーズを付与した画像から、その視点外からの新視点の画像を推定するタスクである。
SfMやMVSがGeometric Reconstructionとするならば、新規視点合成はAppearance Reconstructionとなる。
Appearanceには当然、Geometryの情報も含まれているため、Appearance ReconstructionはGeometry Reconstructionのタスクを暗に含んでいると言える。
図を挿入して上げるぞ。(CGのレンダリングプロセスと、ベン図)


\subsubsection{Neural Radiance Fields (NeRF)}
微分可能レンダリングの代表例が NeRF (Neural Radiance Fields) である。

\missingfigure{NeRFと3DGSの図。レイトレーシングとラスタライズの違いを表すイメージ。Survey論文に合った。}

NeRFでは、シーンをボクセルやポリゴンといった明示的な幾何構造ではなく、多層パーセプトロン（MLP）を用いた「連続的な輝度場（Radiance Field）」として表現する。
これは、点群やメッシュといった、シーンの幾何情報を直接的にエンコードするExplicit Representationではなく、関数を介して表現するImplicit Representationの一種である。

\missingfigure{NeRFのMLPの構造も図で載せたい。引用できる}

具体的には、空間上の座標 $(x, y, z)$ と視線方向 $(\theta, \phi)$ を入力とし、その点における放射輝度（RGB）と体積密度（$\sigma$）を出力する関数 $f_\Theta$ を学習する。
この表現を用いて、以下の Volumetric Renderingによりピクセル値を算出する。

\begin{equation}
    C(\mathbf{r}) = \int_{t_n}^{t_f} T(t) \sigma(\mathbf{r}(t)) \mathbf{c}(\mathbf{r}(t), \mathbf{d}) dt
\end{equation}

ここで $T(t)$ は透過率、$\mathbf{r}(t)$ はレイ上の点を表す。この積分プロセスは離散的なサンプリングによって近似され、微分可能な形で実装される。
ボリュメトリックレンダリングは、元来、煙や炎などの不均等な媒質を通過する光の伝播をモデル化を可視化するために開発されたものだが、メッシュなどのラスタライズに含まれる離散性がなく、微分可能性を保つとともに、汎用的なScene表現に用いられる。
これにより、複雑な光学的特性（反射や半透明など）を含むシーンにおいても、エンドツーエンドでの高精度な再構成が可能となった。

ピクセル色の決定には、古典的なボリュームレンダリングの原理が応用される。仮想的なカメラから放たれたレイ（光線）に沿って空間をサンプリングし、各サンプル点での密度 $\sigma$ を重みとした放射輝度 $\mathbf{c}$ の積分値を算出することで、最終的なピクセル値が決定される。
この積分計算は離散的な総和として近似されるが、演算過程の全てが微分可能に保たれているため、レンダリング画像と実画像の二乗誤差を損失関数とし、MLPの重みをエンドツーエンドで最適化できる。
しかし、NeRFは写真のような忠実度の自由視点合成を実現した一方で、実用上の重大な障壁も露呈させた。
1ピクセルの描画のためにレイに沿った数百回のMLP推論を要する計算負荷の高さは、リアルタイムレンダリングを困難にし、またシーンごとに日単位の学習を要する点は、大規模なデータセットへの適用を制限している。
また、3次元シーンがMLPにエンコードされる点で、その解釈や編集可能性が乏しく、測量用途としての適用性に欠ける。
これらの課題、すなわち「計算資源の集約性」と「暗黙的表現による編集の困難性」を克服しようとする動機が、後の明示的な幾何プリミティブを用いる手法への回帰を促すこととなった。
